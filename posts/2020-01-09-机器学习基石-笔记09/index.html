<!DOCTYPE html>
<html lang="zh-Hans-CN">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		
		<meta name="author" content="Simeon">
		<meta name="description" content="Simeon 个人微博">
		<meta name="generator" content="Hugo 0.52" />
		<title>机器学习基石-笔记09 &middot; Simeon&#39;s blog</title>
		<link rel="shortcut icon" href="https://simeon49.github.io/blog/images/favicon.ico">
		<link rel="stylesheet" href="https://simeon49.github.io/blog/css/style.css">
		<link rel="stylesheet" href="https://simeon49.github.io/blog/css/highlight.css">

		
		<link rel="stylesheet" href="https://simeon49.github.io/blog/css/font-awesome.min.css">
		

		

		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://simeon49.github.io/blog/'> <span class="arrow">←</span>Home</a>
	
	<a href='https://simeon49.github.io/blog/posts'>Archive</a>
	<a href='https://simeon49.github.io/blog/tags'>Tags</a>
	<a href='https://simeon49.github.io/blog/categories'>Categories</a>
	<a href='https://simeon49.github.io/blog/about'>About</a>

	

	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        机器学习基石-笔记09
                    </h1>
                    <h2 class="headline">
                    Jan 9, 2020 00:00
                    · 2843 words
                    · 6 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://simeon49.github.io/blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
                          
                              <a href="https://simeon49.github.io/blog/tags/ml">ML</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    

<p>NTU林轩田的《机器学习基石》课程笔记(转载) <br>
&hellip;</p>

<h1 id="9-linear-regression">9 &ndash; Linear Regression</h1>

<p>上节课，我们主要介绍了在有noise的情况下，VC Bound理论仍然是成立的。同时，介绍了不同的error measure方法。本节课介绍机器学习最常见的一种算法：Linear Regression.</p>

<h3 id="一-线性回归问题"><strong>一、线性回归问题</strong></h3>

<p>在之前的Linear Classification课程中，讲了信用卡发放的例子，利用机器学习来决定是否给用户发放信用卡。本节课仍然引入信用卡的例子，来解决给用户发放信用卡额度的问题，这就是一个线性回归（Linear Regression）问题。</p>

<p><img src="../../pic/ML/c72c0eda29fa5c940a85001c0a40ba1d.jpg" alt="这里写图片描述" /></p>

<p>令用户特征集为d维的<img src="../../pic/ML/60ef7ffc7ee9f8c9f54a54940001c8b9.jpg" alt="" />，加上常数项，维度为<img src="../../pic/ML/42e98d2659cfeeeccf8e0918c54734e6.jpg" alt="" />，与权重<img src="../../pic/ML/f12c5a9ac27d1343c1771a3ae3bdd808.jpg" alt="" />的线性组合即为Hypothesis,记为<img src="../../pic/ML/a9f755cd0c77c33b451f9ba084291cd0.jpg" alt="" />。线性回归的预测函数取值在整个实数空间，这跟线性分类不同。</p>

<p><img src="../../pic/ML/6872c83bee6f736a9cbad525406cbf8c.jpg" alt="这里写图片描述" /></p>

<p>根据上图，在一维或者多维空间里，线性回归的目标是找到一条直线（对应一维）、一个平面（对应二维）或者更高维的超平面，使样本集中的点更接近它，也就是残留误差Residuals最小化。</p>

<p>一般最常用的错误测量方式是基于最小二乘法，其目标是计算误差的最小平方和对应的权重w，即上节课介绍的squared error：</p>

<p><img src="../../pic/ML/0ed74ab22a9571be1d368ff97a16cb10.jpg" alt="这里写图片描述" /></p>

<p>这里提一点，最小二乘法可以解决线性问题和非线性问题。线性最小二乘法的解是closed-form，即<img src="../../pic/ML/60ef7ffc7ee9f8c9f54a54940001c8b9.jpg" alt="" />，而非线性最小二乘法没有closed-form，通常用迭代法求解。本节课的解就是closed-form的。关于最小二乘法的一些介绍，请参见我的另一篇博文：</p>

<p><a href="http://blog.csdn.net/red_stone1/article/details/70306403">最小二乘法和梯度下降法的一些总结</a></p>

<h3 id="二-线性回归算法"><strong>二、线性回归算法</strong></h3>

<p>样本数据误差<img src="../../pic/ML/0b0efbc51f6174787220f3866f14ec85.jpg" alt="" />是权重<img src="../../pic/ML/f12c5a9ac27d1343c1771a3ae3bdd808.jpg" alt="" />的函数，因为<img src="../../pic/ML/60ef7ffc7ee9f8c9f54a54940001c8b9.jpg" alt="" />和<img src="../../pic/ML/3977bbae8ef99d4eef1ec15793c43a49.jpg" alt="" />都是已知的。我们的目标就是找出合适的<img src="../../pic/ML/f12c5a9ac27d1343c1771a3ae3bdd808.jpg" alt="" />，使<img src="../../pic/ML/0b0efbc51f6174787220f3866f14ec85.jpg" alt="" />能够最小。那么如何计算呢？</p>

<p>首先，运用矩阵转换的思想，将<img src="../../pic/ML/0b0efbc51f6174787220f3866f14ec85.jpg" alt="" />计算转换为矩阵的形式。</p>

<p><img src="../../pic/ML/1b7aeebc9e8b9f00f8e25d37f34148b7.jpg" alt="这里写图片描述" /></p>

<p>然后，对于此类线性回归问题，<img src="../../pic/ML/0b0efbc51f6174787220f3866f14ec85.jpg" alt="" />一般是个凸函数。凸函数的话，我们只要找到一阶导数等于零的位置，就找到了最优解。那么，我们将<img src="../../pic/ML/1a3f08969fcb7fcf103ea7390e080cbb.jpg" alt="" />对每个<img src="../../pic/ML/f12c5a9ac27d1343c1771a3ae3bdd808.jpg" alt="" />求偏导，偏导为零的<img src="../../pic/ML/f12c5a9ac27d1343c1771a3ae3bdd808.jpg" alt="" />，即为最优化的权重值分布。</p>

<p><img src="../../pic/ML/6ba3f882240cc725877d5d014556a5f8.jpg" alt="这里写图片描述" /></p>

<p>根据梯度的思想，对<img src="../../pic/ML/1a3f08969fcb7fcf103ea7390e080cbb.jpg" alt="" />进行矩阵话求偏导处理：</p>

<p><img src="../../pic/ML/8f1cd29b15384558023e7a5482efb3d5.jpg" alt="这里写图片描述" /></p>

<p>令偏导为零，最终可以计算出权重向量<img src="../../pic/ML/f12c5a9ac27d1343c1771a3ae3bdd808.jpg" alt="" />为：</p>

<p><img src="../../pic/ML/30587f21ddf5ce888864a307215fb964.jpg" alt="这里写图片描述" /></p>

<p>最终，我们推导得到了权重向量<img src="../../pic/ML/f12c5a9ac27d1343c1771a3ae3bdd808.jpg" alt="" />，这是上文提到的closed-form解。其中，<img src="../../pic/ML/e4de4ad48331112538ad0a3397a245e1.jpg" alt="" />又称为伪逆矩阵pseudo-inverse，记为<img src="../../pic/ML/60ef7ffc7ee9f8c9f54a54940001c8b9.jpg" alt="" />，维度是(d+1)xN。</p>

<p>但是，我们注意到，伪逆矩阵中有逆矩阵的计算，逆矩阵<img src="../../pic/ML/6bfe52a24feb6712b6d9b99b569dd8ac.jpg" alt="" />是否一定存在？一般情况下，只要满足样本数量N远大于样本特征维度d+1，就能保证矩阵的逆是存在的，称之为非奇异矩阵。但是如果是奇异矩阵，不可逆怎么办呢？其实，大部分的计算逆矩阵的软件程序，都可以处理这个问题，也会计算出一个逆矩阵。所以，一般伪逆矩阵是可解的。</p>

<h3 id="三-泛化问题"><strong>三、泛化问题</strong></h3>

<p>现在，可能有这样一个疑问，就是这种求解权重向量的方法是机器学习吗？或者说这种方法满足我们之前推导VC Bound，即是否泛化能力强<img src="../../pic/ML/0b0efbc51f6174787220f3866f14ec85.jpg" alt="" />？</p>

<p><img src="../../pic/ML/f046ec9fd76daa84549ec1323b209b06.jpg" alt="这里写图片描述" /></p>

<p>有两种观点：1、这不属于机器学习范畴。因为这种closed-form解的形式跟一般的机器学习算法不一样，而且在计算最小化误差的过程中没有用到迭代。2、这属于机器学习范畴。因为从结果上看，<img src="../../pic/ML/0b0efbc51f6174787220f3866f14ec85.jpg" alt="" />和<img src="../../pic/ML/b72ebcb54db6e93755e362a2c758e0ab.jpg" alt="" />都实现了最小化，而且实际上在计算逆矩阵的过程中，也用到了迭代。</p>

<p>其实，只从结果来看，这种方法的确实现了机器学习的目的。下面通过介绍一种更简单的方法，证明linear regression问题是可以通过线下最小二乘法方法计算得到好的<img src="../../pic/ML/0b0efbc51f6174787220f3866f14ec85.jpg" alt="" />和<img src="../../pic/ML/b72ebcb54db6e93755e362a2c758e0ab.jpg" alt="" />的。</p>

<p><img src="../../pic/ML/b6bf4e022b3e912aa88d0a8ca58dcba1.jpg" alt="这里写图片描述" /></p>

<p>首先，我们根据平均误差的思想，把<img src="../../pic/ML/0b0efbc51f6174787220f3866f14ec85.jpg" alt="" />写成如图的形式，经过变换得到:</p>

<p>我们称<img src="../../pic/ML/60ef7ffc7ee9f8c9f54a54940001c8b9.jpg" alt="" />为帽子矩阵，用H表示。</p>

<p>下面从几何图形的角度来介绍帽子矩阵H的物理意义。</p>

<p><img src="../../pic/ML/2996b1d3a288bc9e9c4e911827b63d9f.jpg" alt="这里写图片描述" /></p>

<p>图中，y是N维空间的一个向量，粉色区域表示输入矩阵X乘以不同权值向量w所构成的空间，根据所有w的取值，预测输出都被限定在粉色的空间中。向量<img src="../../pic/ML/d703dd9459aded7d1177d1ebd655166f.jpg" alt="" />就是粉色空间中的一个向量，代表预测的一种。y是实际样本数据输出值。</p>

<p>机器学习的目的是在粉色空间中找到一个<img src="../../pic/ML/d703dd9459aded7d1177d1ebd655166f.jpg" alt="" />，使它最接近真实的y，那么我们只要将y在粉色空间上作垂直投影即可，投影得到的<img src="../../pic/ML/d703dd9459aded7d1177d1ebd655166f.jpg" alt="" />即为在粉色空间内最接近y的向量。这样即使平均误差<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />最小。</p>

<p>从图中可以看出，<img src="../../pic/ML/d703dd9459aded7d1177d1ebd655166f.jpg" alt="" />是y的投影，已知<img src="../../pic/ML/d703dd9459aded7d1177d1ebd655166f.jpg" alt="" />，那么H表示的就是将y投影到<img src="../../pic/ML/d703dd9459aded7d1177d1ebd655166f.jpg" alt="" />的一种操作。图中绿色的箭头<img src="../../pic/ML/3977bbae8ef99d4eef1ec15793c43a49.jpg" alt="" />是向量y与<img src="../../pic/ML/d703dd9459aded7d1177d1ebd655166f.jpg" alt="" />相减，<img src="../../pic/ML/3977bbae8ef99d4eef1ec15793c43a49.jpg" alt="" />垂直于粉色区域。已知<img src="../../pic/ML/9ffdfb49e66c0e94c27b49c114576d6c.jpg" alt="" />那么I-H表示的就是将y投影到<img src="../../pic/ML/3977bbae8ef99d4eef1ec15793c43a49.jpg" alt="" />即垂直于粉色区域的一种操作。这样的话，我们就赋予了H和I-H不同但又有联系的物理意义。</p>

<p>这里trace(I-H)称为I-H的迹，值为N-(d+1)。这条性质很重要，一个矩阵的 trace等于该矩阵的所有特征值(Eigenvalues)之和。下面给出简单证明：</p>

<p><img src="../../pic/ML/259a8ff675911e046d50624b0f264100.jpg" alt="" />
<img src="../../pic/ML/73f675cb02947f18734259a6ba136c82.jpg" alt="" />
<img src="../../pic/ML/1b7ae0053f3d9a51bd9a6e8da7189d50.jpg" alt="" />
<img src="../../pic/ML/a95e59395b14a2cf3d94915afbcf570a.jpg" alt="" /></p>

<p>介绍下该I-H这种转换的物理意义：原来有一个有N个自由度的向量y，投影到一个有d+1维的空间x（代表一列的自由度，即单一输入样本的参数，如图中粉色区域），而余数剩余的自由度最大只有N-(d+1)种。</p>

<p>在存在noise的情况下，上图变为：</p>

<p><img src="../../pic/ML/56a536ccdd11a635b6ceaeab7eb56b0d.jpg" alt="这里写图片描述" /></p>

<p>图中，粉色空间的红色箭头是目标函数f(x)，虚线箭头是noise，可见，真实样本输出y由f(x)和noise相加得到。由上面推导，已知向量y经过I-H转换为<img src="../../pic/ML/3977bbae8ef99d4eef1ec15793c43a49.jpg" alt="" />，而noise与y是线性变换关系，那么根据线性函数知识，我们推导出noise经过I-H也能转换为<img src="../../pic/ML/3977bbae8ef99d4eef1ec15793c43a49.jpg" alt="" />。则对于样本平均误差，有下列推导成立：</p>

<p>即</p>

<p>同样，对<img src="../../pic/ML/b72ebcb54db6e93755e362a2c758e0ab.jpg" alt="" />有如下结论：</p>

<p>这个证明有点复杂，但是我们可以这样理解：<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />与<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />形式上只差了<img src="../../pic/ML/731e77bb5a621e147548bb15adc2fd53.jpg" alt="" />项，从哲学上来说，<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />是我们看得到的样本的平均误差，如果有noise，我们把预测往noise那边偏一点，让<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />好看一点点，所以减去<img src="../../pic/ML/731e77bb5a621e147548bb15adc2fd53.jpg" alt="" />项。那么同时，新的样本<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />是我们看不到的，如果noise在反方向，那么<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />就应该加上<img src="../../pic/ML/731e77bb5a621e147548bb15adc2fd53.jpg" alt="" />项。</p>

<p>我们把<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />与<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />画出来，得到学习曲线：</p>

<p><img src="../../pic/ML/87b2f843a95e1a5bdead402256110413.jpg" alt="这里写图片描述" /></p>

<p>当N足够大时，<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />与<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />逐渐接近，满足<img src="../../pic/ML/f7965e606c453854be7ff54494b5b6f0.jpg" alt="" />，且数值保持在noise level。这就类似VC理论，证明了当N足够大的时候，这种线性最小二乘法是可以进行机器学习的，算法有效！</p>

<h3 id="四-linear-regression方法解决linear-classification问题"><strong>四、Linear Regression方法解决Linear Classification问题</strong></h3>

<p>之前介绍的Linear Classification问题使用的Error Measure方法用的是0/1 error，那么Linear Regression的squared error是否能够应用到Linear Classification问题？</p>

<p><img src="../../pic/ML/2fb89c1cd7596fae9133445344506e1d.jpg" alt="这里写图片描述" /></p>

<p>下图展示了两种错误的关系，一般情况下，squared error曲线在0/1 error曲线之上。即<img src="../../pic/ML/a0f84e990fb192032733a4e2d0f3682c.jpg" alt="" />.</p>

<p><img src="../../pic/ML/1bbb3f870c17b34992ad7649ada15597.jpg" alt="这里写图片描述" /></p>

<p>根据之前的VC理论，<img src="../../pic/ML/b72ebcb54db6e93755e362a2c758e0ab.jpg" alt="" />的上界满足：</p>

<p><img src="../../pic/ML/8ac66be2c22433316a9fbc085ec8d6dc.jpg" alt="这里写图片描述" /></p>

<p>从图中可以看出，用<img src="../../pic/ML/00dff8bf6e32c56fa828c55e40b4c44f.jpg" alt="" />代替<img src="../../pic/ML/b406a8f466b66fbe008d1af8dfe5240b.jpg" alt="" />，<img src="../../pic/ML/b72ebcb54db6e93755e362a2c758e0ab.jpg" alt="" />仍然有上界，只不过是上界变得宽松了。也就是说用线性回归方法仍然可以解决线性分类问题，效果不会太差。二元分类问题得到了一个更宽松的上界，但是也是一种更有效率的求解方式。</p>

<h3 id="五-总结"><strong>五、总结</strong></h3>

<p>本节课，我们主要介绍了Linear Regression。首先，我们从问题出发，想要找到一条直线拟合实际数据值；然后，我们利用最小二乘法，用解析形式推导了权重w的closed-form解；接着，用图形的形式得到<img src="../../pic/ML/b72ebcb54db6e93755e362a2c758e0ab.jpg" alt="" />，证明了linear regression是可以进行机器学习的，；最后，我们证明linear regressin这种方法可以用在binary classification上，虽然上界变宽松了，但是仍然能得到不错的学习方法。</p>

<p><strong><em>注明：</em></strong></p>

<p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>

                </section>
            </article>

            

            
                <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'simeon49';

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';

        
        var disquesLoaded = false;
        dsq.onload = function () {
            disquesLoaded = true;
        };
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

        setTimeout(() => {
            if (!disquesLoaded) {
                alert('加载disques 失败!');
            }
        }, 3000);

    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

            

            
                <ul id="post-list" class="archive readmore">
    <h3>Read more</h3>

    
    
    
        <li>
            <a href="/blog/posts/2020-01-08-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B008/">机器学习基石-笔记08<aside class="dates">Jan 8 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-07-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B007/">机器学习基石-笔记07<aside class="dates">Jan 7 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-06-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B006/">机器学习基石-笔记06<aside class="dates">Jan 6 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-05-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B005/">机器学习基石-笔记05<aside class="dates">Jan 5 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B004/">机器学习基石-笔记04<aside class="dates">Jan 4 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B003/">机器学习基石-笔记03<aside class="dates">Jan 3 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B002/">机器学习基石-笔记02<aside class="dates">Jan 2 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B001/">机器学习基石-笔记01<aside class="dates">Jan 1 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2019-05-10-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%86%85%E5%AE%B9%E5%A4%8D%E4%B9%A007-mysql%E7%9A%84%E8%A1%8C%E7%BA%A7%E9%94%81%E5%85%B1%E4%BA%AB%E9%94%81%E4%B8%8E%E6%8E%92%E5%AE%83%E9%94%81/">数据库内容复习07-MySql的行级锁共享锁与排它锁<aside class="dates">May 10 2019</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2019-04-12-web-component/">web-component<aside class="dates">Apr 12 2019</aside></a>
        </li>
    
</ul>

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://github.com/simeon49">
        <i class="fa fa-github-square"></i>
    </a>
    


</div>

    
    <p class="small">
    
       © Copyright 2020 <i class="fa fa-heart" aria-hidden="true"></i> Simeon
    
    </p>
    
</footer>

        </section>

        <script src="https://simeon49.github.io/blog/js/jquery-3.3.1.min.js"></script>
<script src="https://simeon49.github.io/blog/js/main.js"></script>
<script src="https://simeon49.github.io/blog/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>






<script>
var baiduAnalytics = '74fafdde017951e1df78c761e7c017bc';
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?" + baiduAnalytics;
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    </body>
</html>
