<!DOCTYPE html>
<html lang="zh-Hans-CN">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		
		<meta name="author" content="Simeon">
		<meta name="description" content="Simeon 个人微博">
		<meta name="generator" content="Hugo 0.52" />
		<title>机器学习基石-笔记02 &middot; Simeon&#39;s blog</title>
		<link rel="shortcut icon" href="https://simeon49.github.io/blog/images/favicon.ico">
		<link rel="stylesheet" href="https://simeon49.github.io/blog/css/style.css">
		<link rel="stylesheet" href="https://simeon49.github.io/blog/css/highlight.css">

		
		<link rel="stylesheet" href="https://simeon49.github.io/blog/css/font-awesome.min.css">
		

		

		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://simeon49.github.io/blog/'> <span class="arrow">←</span>Home</a>
	
	<a href='https://simeon49.github.io/blog/posts'>Archive</a>
	<a href='https://simeon49.github.io/blog/tags'>Tags</a>
	<a href='https://simeon49.github.io/blog/categories'>Categories</a>
	<a href='https://simeon49.github.io/blog/about'>About</a>

	

	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        机器学习基石-笔记02
                    </h1>
                    <h2 class="headline">
                    Jan 2, 2020 00:00
                    · 3495 words
                    · 7 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://simeon49.github.io/blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
                          
                              <a href="https://simeon49.github.io/blog/tags/ml">ML</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    

<p>NTU林轩田的《机器学习基石》课程笔记(转载) <br>
&hellip;</p>

<h1 id="2-learning-to-answer-yes-no">2 &ndash; Learning to Answer Yes/No</h1>

<p>上节课，我们主要简述了机器学习的定义及其重要性，并用流程图的形式介绍了机器学习的整个过程：根据模型H，使用演算法A，在训练样本D上进行训练，得到最好的h，其对应的g就是我们最后需要的机器学习的模型函数，一般g接近于目标函数f。本节课将继续深入探讨机器学习问题，介绍感知机Perceptron模型，并推导课程的第一个机器学习算法：Perceptron Learning Algorithm（PLA）。</p>

<h3 id="一-perceptron-hypothesis-set"><strong>一、Perceptron Hypothesis Set</strong></h3>

<p>引入这样一个例子：某银行要根据用户的年龄、性别、年收入等情况来判断是否给该用户发信用卡。现在有训练样本D，即之前用户的信息和是否发了信用卡。这是一个典型的机器学习问题，我们要根据D，通过A，在H中选择最好的h，得到g，接近目标函数f，也就是根据先验知识建立是否给用户发信用卡的模型。银行用这个模型对以后用户进行判断：发信用卡（+1），不发信用卡（-1）。</p>

<p>在这个机器学习的整个流程中，有一个部分非常重要：就是模型选择，即Hypothesis Set。选择什么样的模型，很大程度上会影响机器学习的效果和表现。下面介绍一个简单常用的Hypothesis Set：感知机（Perceptron）。</p>

<p>还是刚才银行是否给用户发信用卡的例子，我们把用户的个人信息作为特征向量x，令总共有d个特征，每个特征赋予不同的权重w，表示该特征对输出（是否发信用卡）的影响有多大。那所有特征的加权和的值与一个设定的阈值threshold进行比较：大于这个阈值，输出为+1，即发信用卡；小于这个阈值，输出为-1，即不发信用卡。感知机模型，就是当特征加权和与阈值的差大于或等于0，则输出h(x)=1；当特征加权和与阈值的差小于0，则输出h(x)=-1，而我们的目的就是计算出所有权值w和阈值threshold。</p>

<p><img src="../../pic/ML/b82f5722a3ab38fa777a0941ce313102.jpg" alt="这里写图片描述" /></p>

<p>为了计算方便，通常我们将阈值threshold当做<img src="../../pic/ML/80aba44293dd8bc07d580471772db0d3.jpg" alt="" />，引入一个<img src="../../pic/ML/ba8e90ec3b50f66d895b239f68d3b97e.jpg" alt="" />的量与<img src="../../pic/ML/80aba44293dd8bc07d580471772db0d3.jpg" alt="" />相乘，这样就把threshold也转变成了权值<img src="../../pic/ML/80aba44293dd8bc07d580471772db0d3.jpg" alt="" />，简化了计算。h(x)的表达式做如下变换：</p>

<p><img src="../../pic/ML/b0388ac66d4e814cf2ecc1c4aebc2799.jpg" alt="这里写图片描述" /></p>

<p>为了更清晰地说明感知机模型，我们假设Perceptrons在二维平面上，即<img src="../../pic/ML/9da10bd554efbaa3d9f62efed8cb7834.jpg" alt="" />。其中，<img src="../../pic/ML/80aba44293dd8bc07d580471772db0d3.jpg" alt="" />是平面上一条分类直线，直线一侧是正类（+1），直线另一侧是负类（-1）。权重w不同，对应于平面上不同的直线。</p>

<p><img src="../../pic/ML/6eb760b955af84cb1123dd680eb4b559.jpg" alt="这里写图片描述" /></p>

<p>那么，我们所说的Perceptron，在这个模型上就是一条直线，称之为linear(binary) classifiers。注意一下，感知器线性分类不限定在二维空间中，在3D中，线性分类用平面表示，在更高维度中，线性分类用超平面表示，即只要是形如<img src="../../pic/ML/b060a4879c8127c91fe14a0a743ca23d.jpg" alt="" />的线性模型就都属于linear(binary) classifiers。</p>

<p>同时，需要注意的是，这里所说的linear(binary) classifiers是用简单的感知器模型建立的，线性分类问题还可以使用logistic regression来解决，后面将会介绍。</p>

<h3 id="二-perceptron-learning-algorithm-pla"><strong>二、Perceptron Learning Algorithm(PLA)</strong></h3>

<p>根据上一部分的介绍，我们已经知道了hypothesis set由许多条直线构成。接下来，我们的目的就是如何设计一个演算法A，来选择一个最好的直线，能将平面上所有的正类和负类完全分开，也就是找到最好的g，使<img src="../../pic/ML/061263d047d98f81c1e18f8f49c57003.jpg" alt="" />。</p>

<p>如何找到这样一条最好的直线呢？我们可以使用逐点修正的思想，首先在平面上随意取一条直线，看看哪些点分类错误。然后开始对第一个错误点就行修正，即变换直线的位置，使这个错误点变成分类正确的点。接着，再对第二个、第三个等所有的错误分类点就行直线纠正，直到所有的点都完全分类正确了，就得到了最好的直线。这种“逐步修正”，就是PLA思想所在。</p>

<p><img src="../../pic/ML/75217f1e29be523c4e5b655d738984a2.jpg" alt="这里写图片描述" /></p>

<p>下面介绍一下PLA是怎么做的。首先随机选择一条直线进行分类。然后找到第一个分类错误的点，如果这个点表示正类，被误分为负类，即<img src="../../pic/ML/4042fc2c320d85292df45d0c2633965e.jpg" alt="" />，那表示w和x夹角大于90度，其中w是直线的法向量。所以，x被误分在直线的下侧（相对于法向量，法向量的方向即为正类所在的一侧），修正的方法就是使w和x夹角小于90度。通常做法是<img src="../../pic/ML/cdb3c7bd482f1beb34c202eef937eefd.jpg" alt="" />，如图右上角所示，一次或多次更新后的<img src="../../pic/ML/9cfd891e8acb93d51d15087897f73aa2.jpg" alt="" />与x夹角小于90度，能保证x位于直线的上侧，则对误分为负类的错误点完成了直线修正。</p>

<p>同理，如果是误分为正类的点，即<img src="../../pic/ML/7e16316a1d9c7fd686cdb66b9f09f630.jpg" alt="" />，那表示w和x夹角小于90度，其中w是直线的法向量。所以，x被误分在直线的上侧，修正的方法就是使w和x夹角大于90度。通常做法是<img src="../../pic/ML/25df598a47ca943cce43cee3f072d4e9.jpg" alt="" />，如图右下角所示，一次或多次更新后的<img src="../../pic/ML/9cfd891e8acb93d51d15087897f73aa2.jpg" alt="" />与x夹角大于90度，能保证x位于直线的下侧，则对误分为正类的错误点也完成了直线修正。</p>

<p>按照这种思想，遇到个错误点就进行修正，不断迭代。要注意一点：每次修正直线，可能使之前分类正确的点变成错误点，这是可能发生的。但是没关系，不断迭代，不断修正，最终会将所有点完全正确分类（PLA前提是线性可分的）。这种做法的思想是“知错能改”，有句话形容它：“A fault confessed is half redressed.”</p>

<p>实际操作中，可以一个点一个点地遍历，发现分类错误的点就进行修正，直到所有点全部分类正确。这种被称为Cyclic PLA。</p>

<p><img src="../../pic/ML/a01f86d78da3a4c18e65c580dcbc3034.jpg" alt="这里写图片描述" /></p>

<p>下面用图解的形式来介绍PLA的修正过程：</p>

<p><img src="../../pic/ML/e0d499510e79ae414ba6d8fd0e2b8873.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/4c2ee658095b1de81eef6462835ccff5.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/eade122d2714fd8f022ed5750dac7ebb.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/ee9dfb1122ece819e36dfa0dffa3d710.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/0907de3645464656b10156121be5ca29.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/3588fd87a025943f3a44dda267611783.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/a0efb902fa72d856e691be2b58d367c8.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/31868f4e7c73a3c6172e1feb72168f5f.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/e54782823ec6e1465008d4c91cdc7924.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/83392b3b4e2f3481d33b79efaed3ec11.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/a7e3b39ad0a959c6a37bc3b4ab350504.jpg" alt="这里写图片描述" /></p>

<p>对PLA，我们需要考虑以下两个问题：</p>

<ul>
<li><p>PLA迭代一定会停下来吗？如果线性不可分怎么办？</p></li>

<li><p>PLA停下来的时候，是否能保证<img src="../../pic/ML/4e89dc8628aa25992aee4c27f454472b.jpg" alt="" />？如果没有停下来，是否有<img src="../../pic/ML/4e89dc8628aa25992aee4c27f454472b.jpg" alt="" />？</p></li>
</ul>

<h3 id="三-guarantee-of-pla"><strong>三、Guarantee of PLA</strong></h3>

<p>PLA什么时候会停下来呢？根据PLA的定义，当找到一条直线，能将所有平面上的点都分类正确，那么PLA就停止了。要达到这个终止条件，就必须保证D是线性可分（linear separable）。如果是非线性可分的，那么，PLA就不会停止。</p>

<p><img src="../../pic/ML/eed8a5d3f418163842ee06008346de06.jpg" alt="这里写图片描述" /></p>

<p>对于线性可分的情况，如果有这样一条直线，能够将正类和负类完全分开，令这时候的目标权重为<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />，则对每个点，必然满足<img src="../../pic/ML/e70d3b173167e8a06cb87df08df81c73.jpg" alt="" />，即对任一点：</p>

<p><img src="../../pic/ML/e13d51359e00687861a9c469e7e0f8d8.jpg" alt="这里写图片描述" /></p>

<p>PLA会对每次错误的点进行修正，更新权重<img src="../../pic/ML/75cc738213b9d738d2f3b8b323d07aef.jpg" alt="" />的值，如果<img src="../../pic/ML/75cc738213b9d738d2f3b8b323d07aef.jpg" alt="" />与<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />越来越接近，数学运算上就是内积越大，那表示<img src="../../pic/ML/75cc738213b9d738d2f3b8b323d07aef.jpg" alt="" />是在接近目标权重<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />，证明PLA是有学习效果的。所以，我们来计算<img src="../../pic/ML/75cc738213b9d738d2f3b8b323d07aef.jpg" alt="" />与<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />的内积：</p>

<p><img src="../../pic/ML/522dc5f7b352e150c9704f00ad795cca.jpg" alt="这里写图片描述" /></p>

<p>从推导可以看出，<img src="../../pic/ML/75cc738213b9d738d2f3b8b323d07aef.jpg" alt="" />与<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />的内积跟<img src="../../pic/ML/2a7c3c74c112ab824335ff317c785f4e.jpg" alt="" />与<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />的内积相比更大了。似乎说明了<img src="../../pic/ML/75cc738213b9d738d2f3b8b323d07aef.jpg" alt="" />更接近<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />，但是内积更大，可能是向量长度更大了，不一定是向量间角度更小。所以，下一步，我们还需要证明<img src="../../pic/ML/75cc738213b9d738d2f3b8b323d07aef.jpg" alt="" />与<img src="../../pic/ML/2a7c3c74c112ab824335ff317c785f4e.jpg" alt="" />向量长度的关系：</p>

<p><img src="../../pic/ML/d316f70f72b7ea87e40817e2cd9f5543.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/2a7c3c74c112ab824335ff317c785f4e.jpg" alt="" />只会在分类错误的情况下更新，最终得到的<img src="../../pic/ML/be9452610f340ddcd227d32452da883b.jpg" alt="" />相比<img src="../../pic/ML/392d2051d8acd806804208c33704175a.jpg" alt="" />的增量值不超过<img src="../../pic/ML/46294e55f4b6c554e691470ae7ec3d2a.jpg" alt="" />。也就是说，<img src="../../pic/ML/2a7c3c74c112ab824335ff317c785f4e.jpg" alt="" />的增长被限制了，<img src="../../pic/ML/75cc738213b9d738d2f3b8b323d07aef.jpg" alt="" />与<img src="../../pic/ML/2a7c3c74c112ab824335ff317c785f4e.jpg" alt="" />向量长度不会差别太大！</p>

<p>如果令初始权值<img src="../../pic/ML/80aba44293dd8bc07d580471772db0d3.jpg" alt="" />，那么经过T次错误修正后，有如下结论：</p>

<p>下面贴出来该结论的具体推导过程：</p>

<p><img src="../../pic/ML/9499cd5bc28ac0c3d524f8115ef09a46.jpg" alt="这里写图片描述" /></p>

<p><img src="../../pic/ML/182b2ba169926a57c9bb35d350c6d04f.jpg" alt="这里写图片描述" /></p>

<p>上述不等式左边其实是<img src="../../pic/ML/2ed3b41c02e21f3224f54ab912153516.jpg" alt="" />与<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />夹角的余弦值，随着T增大，该余弦值越来越接近1，即<img src="../../pic/ML/2ed3b41c02e21f3224f54ab912153516.jpg" alt="" />与<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />越来越接近。同时，需要注意的是，<img src="../../pic/ML/2b1582bb9dd36767edc8638d5142f4a6.jpg" alt="" />，也就是说，迭代次数T是有上界的。根据以上证明，我们最终得到的结论是：<img src="../../pic/ML/75cc738213b9d738d2f3b8b323d07aef.jpg" alt="" />与<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />的是随着迭代次数增加，逐渐接近的。而且，PLA最终会停下来（因为T有上界），实现对线性可分的数据集完全分类。</p>

<h3 id="四-non-separable-data"><strong>四、Non-Separable Data</strong></h3>

<p>上一部分，我们证明了线性可分的情况下，PLA是可以停下来并正确分类的，但对于非线性可分的情况，<img src="../../pic/ML/40de538fbc9692d37d7d1b7ab8958efb.jpg" alt="" />实际上并不存在，那么之前的推导并不成立，PLA不一定会停下来。所以，PLA虽然实现简单，但也有缺点：</p>

<p><img src="../../pic/ML/184006fbd0c94062a9bad5aecdb2b705.jpg" alt="这里写图片描述" /></p>

<p>对于非线性可分的情况，我们可以把它当成是数据集D中掺杂了一下noise，事实上，大多数情况下我们遇到的D，都或多或少地掺杂了noise。这时，机器学习流程是这样的：</p>

<p><img src="../../pic/ML/5ca498908766c95678b857366c359ede.jpg" alt="这里写图片描述" /></p>

<p>在非线性情况下，我们可以把条件放松，即不苛求每个点都分类正确，而是容忍有错误点，取错误点的个数最少时的权重w：</p>

<p><img src="../../pic/ML/459e9ae9fb763c5285e4b3d6e7017446.jpg" alt="这里写图片描述" /></p>

<p>事实证明，上面的解是NP-hard问题，难以求解。然而，我们可以对在线性可分类型中表现很好的PLA做个修改，把它应用到非线性可分类型中，获得近似最好的g。</p>

<p>修改后的PLA称为Packet Algorithm。它的算法流程与PLA基本类似，首先初始化权重<img src="../../pic/ML/80aba44293dd8bc07d580471772db0d3.jpg" alt="" />，计算出在这条初始化的直线中，分类错误点的个数。然后对错误点进行修正，更新w，得到一条新的直线，在计算其对应的分类错误的点的个数，并与之前错误点个数比较，取个数较小的直线作为我们当前选择的分类直线。之后，再经过n次迭代，不断比较当前分类错误点个数与之前最少的错误点个数比较，选择最小的值保存。直到迭代次数完成后，选取个数最少的直线对应的w，即为我们最终想要得到的权重值。</p>

<p><img src="../../pic/ML/5d5060a872bbddfe2d5fb0263407ee05.jpg" alt="这里写图片描述" /></p>

<p>如何判断数据集D是不是线性可分？对于二维数据来说，通常还是通过肉眼观察来判断的。一般情况下，Pocket Algorithm要比PLA速度慢一些。</p>

<h3 id="五-总结"><strong>五、总结</strong></h3>

<p>本节课主要介绍了线性感知机模型，以及解决这类感知机分类问题的简单算法：PLA。我们详细证明了对于线性可分问题，PLA可以停下来并实现完全正确分类。对于不是线性可分的问题，可以使用PLA的修正算法Pocket Algorithm来解决。</p>

<p><strong><em>注明：</em></strong></p>

<p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程。</p>

                </section>
            </article>

            

            
                <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'simeon49';

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';

        
        var disquesLoaded = false;
        dsq.onload = function () {
            disquesLoaded = true;
        };
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

        setTimeout(() => {
            if (!disquesLoaded) {
                alert('加载disques 失败!');
            }
        }, 3000);

    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

            

            
                <ul id="post-list" class="archive readmore">
    <h3>Read more</h3>

    
    
    
        <li>
            <a href="/blog/posts/2020-01-09-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B009/">机器学习基石-笔记09<aside class="dates">Jan 9 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-08-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B008/">机器学习基石-笔记08<aside class="dates">Jan 8 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-07-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B007/">机器学习基石-笔记07<aside class="dates">Jan 7 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-06-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B006/">机器学习基石-笔记06<aside class="dates">Jan 6 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-05-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B005/">机器学习基石-笔记05<aside class="dates">Jan 5 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B004/">机器学习基石-笔记04<aside class="dates">Jan 4 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B003/">机器学习基石-笔记03<aside class="dates">Jan 3 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B001/">机器学习基石-笔记01<aside class="dates">Jan 1 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2019-05-10-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%86%85%E5%AE%B9%E5%A4%8D%E4%B9%A007-mysql%E7%9A%84%E8%A1%8C%E7%BA%A7%E9%94%81%E5%85%B1%E4%BA%AB%E9%94%81%E4%B8%8E%E6%8E%92%E5%AE%83%E9%94%81-copy/">数据库内容复习07-MySql的行级锁共享锁与排它锁<aside class="dates">May 10 2019</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2019-04-12-web-component/">web-component<aside class="dates">Apr 12 2019</aside></a>
        </li>
    
</ul>

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://github.com/simeon49">
        <i class="fa fa-github-square"></i>
    </a>
    


</div>

    
    <p class="small">
    
       © Copyright 2020 <i class="fa fa-heart" aria-hidden="true"></i> Simeon
    
    </p>
    
</footer>

        </section>

        <script src="https://simeon49.github.io/blog/js/jquery-3.3.1.min.js"></script>
<script src="https://simeon49.github.io/blog/js/main.js"></script>
<script src="https://simeon49.github.io/blog/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>






<script>
var baiduAnalytics = '74fafdde017951e1df78c761e7c017bc';
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?" + baiduAnalytics;
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    </body>
</html>
