<!DOCTYPE html>
<html lang="zh-Hans-CN">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		
		<meta name="author" content="Simeon">
		<meta name="description" content="Simeon 个人微博">
		<meta name="generator" content="Hugo 0.52" />
		<title>机器学习基石-笔记07 &middot; Simeon&#39;s blog</title>
		<link rel="shortcut icon" href="https://simeon49.github.io/blog/images/favicon.ico">
		<link rel="stylesheet" href="https://simeon49.github.io/blog/css/style.css">
		<link rel="stylesheet" href="https://simeon49.github.io/blog/css/highlight.css">

		
		<link rel="stylesheet" href="https://simeon49.github.io/blog/css/font-awesome.min.css">
		

		

		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://simeon49.github.io/blog/'> <span class="arrow">←</span>Home</a>
	
	<a href='https://simeon49.github.io/blog/posts'>Archive</a>
	<a href='https://simeon49.github.io/blog/tags'>Tags</a>
	<a href='https://simeon49.github.io/blog/categories'>Categories</a>
	<a href='https://simeon49.github.io/blog/about'>About</a>

	

	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        机器学习基石-笔记07
                    </h1>
                    <h2 class="headline">
                    Jan 7, 2020 00:00
                    · 2903 words
                    · 6 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://simeon49.github.io/blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
                          
                              <a href="https://simeon49.github.io/blog/tags/ml">ML</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    

<p>NTU林轩田的《机器学习基石》课程笔记(转载) <br>
&hellip;</p>

<h1 id="7-the-vc-dimension">7 &ndash; The VC Dimension</h1>

<p>前几节课着重介绍了机器能够学习的条件并做了详细的推导和解释。机器能够学习必须满足两个条件：</p>

<ul>
<li><strong>假设空间H的Size M是有限的，即当N足够大的时候，那么对于假设空间中任意一个假设g，<img src="img/f414f6c0d01c45a4656efaecfbcce3e9.jpg" alt="" /></strong>。</li>
<li><strong>利用算法A从假设空间H中，挑选一个g，使<img src="img/e6d085f2f5b3c5ac156b93b931cf1aaf.jpg" alt="" />，则<img src="img/fabc6ef6eee191794e6e61f6b3fa8453.jpg" alt="" /></strong>。</li>
</ul>

<p>这两个条件，正好对应着test和trian两个过程。train的目的是使损失期望<img src="img/e6d085f2f5b3c5ac156b93b931cf1aaf.jpg" alt="" />；test的目的是使将算法用到新的样本时的损失期望也尽可能小，即<img src="img/fabc6ef6eee191794e6e61f6b3fa8453.jpg" alt="" />。</p>

<p>正因为如此，上次课引入了break point，并推导出只要break point存在，则M有上界，一定存在<img src="img/f414f6c0d01c45a4656efaecfbcce3e9.jpg" alt="" />。</p>

<p>本次笔记主要介绍VC Dimension的概念。同时也是总结VC Dimension与<img src="img/e6d085f2f5b3c5ac156b93b931cf1aaf.jpg" alt="" />，<img src="img/fabc6ef6eee191794e6e61f6b3fa8453.jpg" alt="" />，Model Complexity Penalty（下面会讲到）的关系。</p>

<h3 id="一-definition-of-vc-dimension"><strong>一、Definition of VC Dimension</strong></h3>

<p>首先，我们知道如果一个假设空间H有break point k，那么它的成长函数是有界的，它的上界称为Bound function。根据数学归纳法，Bound function也是有界的，且上界为<img src="img/1c802837e2ff8555d27b6c504fa4aaa0.jpg" alt="" />。从下面的表格可以看出，<img src="img/d585e715f76b1770b71b39ec58406698.jpg" alt="" />比B(N,k)松弛很多。</p>

<p><img src="img/65438447a6404dacb7adf69ce1b84c57.jpg" alt="这里写图片描述" /></p>

<p>则根据上一节课的推导，VC bound就可以转换为：</p>

<p><img src="img/1a241a5e97470e36b6e837ca1c6b7c8f.jpg" alt="这里写图片描述" /></p>

<p>这样，不等式只与k和N相关了，一般情况下样本N足够大，所以我们只考虑k值。有如下结论：</p>

<ul>
<li><p><strong>若假设空间H有break point k，且N足够大，则根据VC bound理论，算法有良好的泛化能力</strong></p></li>

<li><p><strong>在假设空间中选择一个矩g，使<img src="img/d89ccd61d9eb513dd34ee152f11e6f1e.jpg" alt="" />，则其在全集数据中的错误率会较低</strong></p></li>
</ul>

<p><img src="img/a5ca7210e5d1a3867c442706075b3364.jpg" alt="这里写图片描述" /></p>

<p>下面介绍一个新的名词：VC Dimension。VC Dimension就是某假设集H能够shatter的最多inputs的个数，即最大完全正确的分类能力。（注意，只要存在一种分布的inputs能够正确分类也满足）。</p>

<p>shatter的英文意思是“粉碎”，也就是说对于inputs的所有情况都能列举出来。例如对N个输入，如果能够将<img src="img/b2c6525e1b53f5de75304b2103b8f5a2.jpg" alt="" />种情况都列出来，则称该N个输入能够被假设集H shatter。</p>

<p>根据之前break point的定义：假设集不能被shatter任何分布类型的inputs的最少个数。则VC Dimension等于break point的个数减一。</p>

<p><img src="img/d9c6888aa7b6a1aeab0b966821eaf7f7.jpg" alt="这里写图片描述" /></p>

<p>现在，我们回顾一下之前介绍的四种例子，它们对应的VC Dimension是多少：</p>

<p><img src="img/68d0bc0cf15ef728557a7a402eda4333.jpg" alt="这里写图片描述" /></p>

<p>用<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />代替k，那么VC bound的问题也就转换为与<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />和N相关了。同时，如果一个假设集H的<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />确定了，则就能满足机器能够学习的第一个条件<img src="img/f414f6c0d01c45a4656efaecfbcce3e9.jpg" alt="" />，与算法、样本数据分布和目标函数都没有关系。</p>

<p><img src="img/494b1b5f42ccd206e34c5bf81d269b8e.jpg" alt="这里写图片描述" /></p>

<h3 id="二-vc-dimension-of-perceptrons"><strong>二、VC Dimension of Perceptrons</strong></h3>

<p>回顾一下我们之前介绍的2D下的PLA算法，已知Perceptrons的k=4，即<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />。根据VC Bound理论，当N足够大的时候，<img src="img/0cb06285659684a121aa7e03b9d43134.jpg" alt="" />。如果找到一个g，使<img src="img/5f7fa28afc832c40a19b20c56cd5497d.jpg" alt="" />，那么就能证明PLA是可以学习的。</p>

<p><img src="img/c39a6ed2181b71cd40194ec64f9df985.jpg" alt="这里写图片描述" /></p>

<p>这是在2D情况下，那如果是多维的Perceptron，它对应的<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />又等于多少呢？</p>

<p>已知在1D Perceptron，<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />，在2D Perceptrons，<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />，那么我们有如下假设：<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />，其中d为维数。</p>

<p>要证明的话，只需分两步证明：</p>

<ul>
<li><img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" /></li>
<li><img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" /></li>
</ul>

<p><img src="img/b77632a86d53ec6e0992ff34b5f47ac3.jpg" alt="这里写图片描述" /></p>

<p>首先证明第一个不等式：<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />。</p>

<p>在d维里，我们只要找到某一类的d+1个inputs可以被shatter的话，那么必然得到<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />。所以，我们有意构造一个d维的矩阵<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />能够被shatter就行。<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />是d维的，有d+1个inputs，每个inputs加上第零个维度的常数项1，得到<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />的矩阵：</p>

<p><img src="img/d343eaf7d931622ab9c5a18bcb2da7c3.jpg" alt="这里写图片描述" /></p>

<p>矩阵中，每一行代表一个inputs，每个inputs是d+1维的，共有d+1个inputs。这里构造的<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />很明显是可逆的。shatter的本质是假设空间H对<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />的所有情况的判断都是对的，即总能找到权重W，满足<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />，<img src="img/402b6f3e239916446e4ddb6dfb487d13.jpg" alt="" />。由于这里我们构造的矩阵<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />的逆矩阵存在，那么d维的所有inputs都能被shatter，也就证明了第一个不等式。</p>

<p><img src="img/846ab1576112dba66499d6bbd45cddd7.jpg" alt="这里写图片描述" /></p>

<p>然后证明第二个不等式：<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />。</p>

<p>在d维里，如果对于任何的d+2个inputs，一定不能被shatter，则不等式成立。我们构造一个任意的矩阵<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />，其包含d+2个inputs，该矩阵有d+1列，d+2行。这d+2个向量的某一列一定可以被另外d+1个向量线性表示，例如对于向量<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />，可表示为：</p>

<p>其中，假设<img src="img/ed4df0177082a678ddda7cf9f1b763cd.jpg" alt="" />，<img src="img/4a42f9a7dcd77596f77d07806401e190.jpg" alt="" />.</p>

<p>那么如果<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />是正类，<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />均为负类，则存在<img src="img/207b8020a44212d6583b9a9a508eaf27.jpg" alt="" />，得到如下表达式：
<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />&lt;font color=&ldquo;#0000ff&rdquo;&gt;<img src="img/87f48f3e65043ed2ce3c7be04fb1b2fa.jpg" alt="" />&lt;/font&gt;+&lt;font color=&ldquo;#ff0000&rdquo;&gt;<img src="img/d15fd89e8ee09367b46f5d67b8deafc2.jpg" alt="" />&lt;/font&gt;+<img src="img/bfcaf05c391bf5e95b0123077b1793bb.jpg" alt="" />+&lt;font color=&ldquo;#ff0000&rdquo;&gt;<img src="img/d59543d0e11f3be964667fa1cd0aa6f1.jpg" alt="" />&lt;/font&gt;<img src="img/e06d0d70da65d0d75d43554b0fcfd918.jpg" alt="" /></p>

<p>因为其中蓝色项大于0，代表正类；红色项小于0，代表负类。所有对于这种情况，<img src="img/1147960c1c23d8dc45d182a5f97785a4.jpg" alt="" />一定是正类，无法得到负类的情况。也就是说，d+2个inputs无法被shatter。证明完毕！</p>

<p><img src="img/7b63d1d73456e3b08d3d8528dd085985.jpg" alt="这里写图片描述" /></p>

<p>综上证明可得<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />。</p>

<h3 id="三-physical-intuition-vc-dimension"><strong>三、Physical Intuition VC Dimension</strong></h3>

<p><img src="img/11bfe1c44d49f8d52587556bd70a9dfb.jpg" alt="这里写图片描述" /></p>

<p>上节公式中<img src="img/207b8020a44212d6583b9a9a508eaf27.jpg" alt="" />又名features，即自由度。自由度是可以任意调节的，如同上图中的旋钮一样，可以调节。VC Dimension代表了假设空间的分类能力，即反映了H的自由度，产生dichotomy的数量，也就等于features的个数，但也不是绝对的。</p>

<p><img src="img/79fb72999312ff07fbe285f50b3a39cc.jpg" alt="这里写图片描述" /></p>

<p>例如，对2D Perceptrons，线性分类，<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />，则<img src="img/207b8020a44212d6583b9a9a508eaf27.jpg" alt="" />，也就是说只要3个features就可以进行学习，自由度为3。</p>

<p>介绍到这，我们发现M与<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />是成正比的，从而得到如下结论：</p>

<p><img src="img/1a5f49dd0cd8d8428ba57e52fdc7e7e3.jpg" alt="这里写图片描述" /></p>

<h3 id="四-interpreting-vc-dimension"><strong>四、Interpreting VC Dimension</strong></h3>

<p>下面，我们将更深入地探讨VC Dimension的意义。首先，把VC Bound重新写到这里：</p>

<p><img src="img/659f73151c15a98ed0d96b332cc9b5b1.jpg" alt="这里写图片描述" /></p>

<p>根据之前的泛化不等式，如果<img src="img/1061e3cbc87053b56ae2b1573a2f6451.jpg" alt="" />，即出现bad坏的情况的概率最大不超过<img src="img/e43bd121c6fef869baa12e9adf4a201a.jpg" alt="" />。那么反过来，对于good好的情况发生的概率最小为<img src="img/b4330fb1bb4c0e475155f0d398d0ce3b.jpg" alt="" />，则对上述不等式进行重新推导：</p>

<p><img src="img/baca48fe5828da7f116ce70f49419886.jpg" alt="这里写图片描述" /></p>

<p><img src="img/a385c3bea2dcb00656993bbdebe340d8.jpg" alt="" />表现了假设空间H的泛化能力，<img src="img/a385c3bea2dcb00656993bbdebe340d8.jpg" alt="" />越小，泛化能力越大。</p>

<p><img src="img/7216f7a0afaea0c4553f33039295dce8.jpg" alt="这里写图片描述" /></p>

<p>至此，已经推导出泛化误差<img src="img/4707f2d200bb617863f7161e0de612c5.jpg" alt="" />的边界，因为我们更关心其上界（<img src="img/4707f2d200bb617863f7161e0de612c5.jpg" alt="" />可能的最大值），即：</p>

<p><img src="img/9fcc49a165eccfae45b7e9feb6800a83.jpg" alt="这里写图片描述" /></p>

<p>上述不等式的右边第二项称为模型复杂度，其模型复杂度与样本数量N、假设空间H(<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />)、<img src="img/a385c3bea2dcb00656993bbdebe340d8.jpg" alt="" />有关。<img src="img/4707f2d200bb617863f7161e0de612c5.jpg" alt="" />由<img src="img/571ed3fcbee703f5e037a62366e1e1d8.jpg" alt="" />共同决定。下面绘出<img src="img/4707f2d200bb617863f7161e0de612c5.jpg" alt="" />、model complexity、<img src="img/571ed3fcbee703f5e037a62366e1e1d8.jpg" alt="" />随<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />变化的关系：</p>

<p><img src="img/ada572ce18dcc85587581450123083f2.jpg" alt="这里写图片描述" /></p>

<p>通过该图可以得出如下结论：</p>

<ul>
<li><p><strong><img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />越大，<img src="img/571ed3fcbee703f5e037a62366e1e1d8.jpg" alt="" />越小，<img src="img/bfd231c1490723e852948a84827e8b04.jpg" alt="" />越大（复杂）</strong>。</p></li>

<li><p><strong><img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />越小，<img src="img/571ed3fcbee703f5e037a62366e1e1d8.jpg" alt="" />越大，<img src="img/bfd231c1490723e852948a84827e8b04.jpg" alt="" />越小（简单）</strong>。</p></li>

<li><p><strong>随着<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />增大，<img src="img/4707f2d200bb617863f7161e0de612c5.jpg" alt="" />会先减小再增大</strong>。</p></li>
</ul>

<p>所以，为了得到最小的<img src="img/4707f2d200bb617863f7161e0de612c5.jpg" alt="" />，不能一味地增大<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />以减小<img src="img/571ed3fcbee703f5e037a62366e1e1d8.jpg" alt="" />，因为<img src="img/571ed3fcbee703f5e037a62366e1e1d8.jpg" alt="" />太小的时候，模型复杂度会增加，造成<img src="img/4707f2d200bb617863f7161e0de612c5.jpg" alt="" />变大。也就是说，选择合适的<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />，选择的features个数要合适。</p>

<p>下面介绍一个概念：样本复杂度（Sample Complexity）。如果选定<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />，样本数据D选择多少合适呢？通过下面一个例子可以帮助我们理解：</p>

<p><img src="img/55ac247926db04a7937ad6bcc1740683.jpg" alt="这里写图片描述" /></p>

<p>通过计算得到N=29300，刚好满足<img src="img/e43bd121c6fef869baa12e9adf4a201a.jpg" alt="" />的条件。N大约是<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />的10000倍。这个数值太大了，实际中往往不需要这么多的样本数量，大概只需要<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />的10倍就够了。N的理论值之所以这么大是因为VC Bound 过于宽松了，我们得到的是一个比实际大得多的上界。</p>

<p><img src="img/2d342e1b5628209372a217cdb60e4177.jpg" alt="这里写图片描述" /></p>

<p>值得一提的是，VC Bound是比较宽松的，而如何收紧它却不是那么容易，这也是机器学习的一大难题。但是，令人欣慰的一点是，VC Bound基本上对所有模型的宽松程度是基本一致的，所以，不同模型之间还是可以横向比较。从而，VC Bound宽松对机器学习的可行性还是没有太大影响。</p>

<h3 id="五-总结"><strong>五、总结</strong></h3>

<p>本节课主要介绍了VC Dimension的概念就是最大的non-break point。然后，我们得到了Perceptrons在d维度下的VC Dimension是d+1。接着，我们在物理意义上，将<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />与自由度联系起来。最终得出结论<img src="img/25e9ecbb8e04e03e89d61c3f36457c57.jpg" alt="" />不能过大也不能过小。选取合适的值，才能让<img src="img/4707f2d200bb617863f7161e0de612c5.jpg" alt="" />足够小，使假设空间H具有良好的泛化能力。</p>

<p><strong><em>注明：</em></strong></p>

<p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>

                </section>
            </article>

            

            
                <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'simeon49';

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';

        
        var disquesLoaded = false;
        dsq.onload = function () {
            disquesLoaded = true;
        };
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

        setTimeout(() => {
            if (!disquesLoaded) {
                alert('加载disques 失败!');
            }
        }, 3000);

    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

            

            
                <ul id="post-list" class="archive readmore">
    <h3>Read more</h3>

    
    
    
        <li>
            <a href="/blog/posts/2020-01-09-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B009/">机器学习基石-笔记09<aside class="dates">Jan 9 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-08-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B008/">机器学习基石-笔记08<aside class="dates">Jan 8 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-06-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B006/">机器学习基石-笔记06<aside class="dates">Jan 6 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-05-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B005/">机器学习基石-笔记05<aside class="dates">Jan 5 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B004/">机器学习基石-笔记04<aside class="dates">Jan 4 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B003/">机器学习基石-笔记03<aside class="dates">Jan 3 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B001/">机器学习基石-笔记01<aside class="dates">Jan 1 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2020-01-02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E7%AC%94%E8%AE%B002/">机器学习基石-笔记01<aside class="dates">Jan 1 2020</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2019-05-10-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%86%85%E5%AE%B9%E5%A4%8D%E4%B9%A007-mysql%E7%9A%84%E8%A1%8C%E7%BA%A7%E9%94%81%E5%85%B1%E4%BA%AB%E9%94%81%E4%B8%8E%E6%8E%92%E5%AE%83%E9%94%81-copy/">数据库内容复习07-MySql的行级锁共享锁与排它锁<aside class="dates">May 10 2019</aside></a>
        </li>
    
        <li>
            <a href="/blog/posts/2019-04-12-web-component/">web-component<aside class="dates">Apr 12 2019</aside></a>
        </li>
    
</ul>

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://github.com/simeon49">
        <i class="fa fa-github-square"></i>
    </a>
    


</div>

    
    <p class="small">
    
       © Copyright 2020 <i class="fa fa-heart" aria-hidden="true"></i> Simeon
    
    </p>
    
</footer>

        </section>

        <script src="https://simeon49.github.io/blog/js/jquery-3.3.1.min.js"></script>
<script src="https://simeon49.github.io/blog/js/main.js"></script>
<script src="https://simeon49.github.io/blog/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>






<script>
var baiduAnalytics = '74fafdde017951e1df78c761e7c017bc';
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?" + baiduAnalytics;
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    </body>
</html>
